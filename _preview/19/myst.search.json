{"version":"1","records":[{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook"},"type":"lvl1","url":"/#arco-era-5-interactive-visualization-cookbook","position":2},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook"},"content":"\n\n\n\n\n\n\n\nA team at \n\nGoogle Research & Cloud are making parts of the \n\nECMWF Reanalysis version 5 (aka ERA-5) accessible in a \n\nAnalysis Ready, Cloud Optimized (aka ARCO) format.\n\nThis Project Pythia Cookbook covers accessing, regridding, and visualizing this reanalysis data.","type":"content","url":"/#arco-era-5-interactive-visualization-cookbook","position":3},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl2":"Motivation"},"type":"lvl2","url":"/#motivation","position":4},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl2":"Motivation"},"content":"The ERA-5 represents the current state-of-the-art meteorological reanalysis, extending from well back into the 20th century to the present. While the data is freely-available for download by archival centers such as \n\nCopernicus and \n\nRDA, the data format and directory structure are typically not well-suited for interactive exploration. Specifically, parameters of interest are stored in individual files, for a limited period of time.\n\nA team at Google Cloud and Research has made interactive exploration much more tenable by representing the data in \n\nZarr format. Each Zarr file represents a specific “class” of meteorological data, such as:\n\nModel Level Wind\n\nModel Level Moisture\n\nSingle Level Surface\n\nSingle Level Reanalysis\n\nSingle Level Forecast\n\nWithin each Zarr file, a variety of meteorological parameters, spanning the current period of record (1 January 1979 --> 31 August 2021) of the ARCO ERA-5 repository exist.\n\nIn the notebooks which comprise this Cookbook, we demonstrate the following:\n\nAccess parameters of interest from the Zarr store\n\nRegrid from model native (Guassian) to lat-lon (Cartesian) coordinates\n\nPlot a map at a specific time using Matplotlib and Cartopy\n\nCreate interactive visualizations, leveraging the \n\nHoloviz ecosystem","type":"content","url":"/#motivation","position":5},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl2":"Authors"},"type":"lvl2","url":"/#authors","position":6},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl2":"Authors"},"content":"Kevin Tyle, \n\nMichael Barletta, \n\nNegin Sobhani, \n\nNicholas Cote , \n\nHarsha Hampapura , and \n\nPhilip Chmielowiec\n\nWe also gratefully acknowledge the Google Cloud Research team for making an ARCO-friendly version of the ERA-5 available. Citations for their effort and the ERA-5 reanalysis are below:\n\nCarver, Robert W, and Merose, Alex. (2023): ARCO-ERA5: An Analysis-Ready Cloud-Optimized Reanalysis Dataset. 22nd Conf. on AI for Env. Science, Denver, CO, Amer. Meteo. Soc, 4A.1, \n\nhttps://​ams​.confex​.com​/ams​/103ANNUAL​/meetingapp​.cgi​/Paper​/415842\n\nHersbach, H., Bell, B., Berrisford, P., Hirahara, S., Horányi, A.,\nMuñoz‐Sabater, J., Nicolas, J., Peubey, C., Radu, R., Schepers, D.,\nSimmons, A., Soci, C., Abdalla, S., Abellan, X., Balsamo, G.,\nBechtold, P., Biavati, G., Bidlot, J., Bonavita, M., De Chiara, G.,\nDahlgren, P., Dee, D., Diamantakis, M., Dragani, R., Flemming, J.,\nForbes, R., Fuentes, M., Geer, A., Haimberger, L., Healy, S.,\nHogan, R.J., Hólm, E., Janisková, M., Keeley, S., Laloyaux, P.,\nLopez, P., Lupu, C., Radnoti, G., de Rosnay, P., Rozum, I., Vamborg, F.,\nVillaume, S., Thépaut, J-N. (2017): Complete ERA5: Fifth generation of\nECMWF atmospheric reanalyses of the global climate. Copernicus Climate\nChange Service (C3S) Data Store (CDS).\nHersbach et al, (2017) was downloaded from the Copernicus Climate Change\nService (C3S) Climate Data Store. We thank C3S for allowing us to\nredistribute the data.\nThe results contain modified Copernicus Climate Change Service\ninformation 2022. Neither the European Commission nor ECMWF is\nresponsible for any use that may be made of the Copernicus information\nor data it contains.","type":"content","url":"/#authors","position":7},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl3":"Contributors","lvl2":"Authors"},"type":"lvl3","url":"/#contributors","position":8},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl3":"Contributors","lvl2":"Authors"},"content":"","type":"content","url":"/#contributors","position":9},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl2":"Structure"},"type":"lvl2","url":"/#structure","position":10},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl2":"Structure"},"content":"This cookbook currently consists of multiple  notebooks that access, regrid, and visualize the ARCO ERA-5 repository. Additionally we cover a section on how to preprocess and create ARCO files.\n\nAdditional notebooks will follow.","type":"content","url":"/#structure","position":11},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl3":"Section 1 ( “Basic Visualization” )","lvl2":"Structure"},"type":"lvl3","url":"/#section-1-basic-visualization","position":12},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl3":"Section 1 ( “Basic Visualization” )","lvl2":"Structure"},"content":"This notebook reads in a sea-level pressure ERA-5 grid, regrids from Gaussian to Cartesian coordinates, and visualizes the data with Matplotlib and Cartopy.","type":"content","url":"/#section-1-basic-visualization","position":13},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl3":"Section 2 ( “Interactive Visualization Part 1: GeoViews” )","lvl2":"Structure"},"type":"lvl3","url":"/#section-2-interactive-visualization-part-1-geoviews","position":14},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl3":"Section 2 ( “Interactive Visualization Part 1: GeoViews” )","lvl2":"Structure"},"content":"This notebook reads in sea-level pressure and 2-meter temperature ERA-5 grids, regrids as in the first notebook, and visualizes the data in an interactive manner using \n\nGeoviews.","type":"content","url":"/#section-2-interactive-visualization-part-1-geoviews","position":15},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl3":"Section 3 (“Interactive Visualization Part 2: hvPlot”)","lvl2":"Structure"},"type":"lvl3","url":"/#section-3-interactive-visualization-part-2-hvplot","position":16},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl3":"Section 3 (“Interactive Visualization Part 2: hvPlot”)","lvl2":"Structure"},"content":"This notebook reads in annual average 2-m temperature from RDA’s Zarr store and visualizes the data using hvPlot. The notebook also demonstrates how to create a simple interactive plot that allows the user to select a specific year and visualize the 2-m and how to create animations.","type":"content","url":"/#section-3-interactive-visualization-part-2-hvplot","position":17},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl3":"Section 4 (“Creating an Interactive Dashboard with Panel”)","lvl2":"Structure"},"type":"lvl3","url":"/#section-4-creating-an-interactive-dashboard-with-panel","position":18},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl3":"Section 4 (“Creating an Interactive Dashboard with Panel”)","lvl2":"Structure"},"content":"This notebook demonstrates how to create an interactive dashboard using Panel that allows the user to select a specific year and visualize the 2-m temperature.","type":"content","url":"/#section-4-creating-an-interactive-dashboard-with-panel","position":19},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl2":"Preprocessing Notebooks for NCAR RDA"},"type":"lvl2","url":"/#preprocessing-notebooks-for-ncar-rda","position":20},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl2":"Preprocessing Notebooks for NCAR RDA"},"content":"","type":"content","url":"/#preprocessing-notebooks-for-ncar-rda","position":21},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl3":"Section 5 ( “Generate annual/yearly Zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive”)","lvl2":"Preprocessing Notebooks for NCAR RDA"},"type":"lvl3","url":"/#section-5-generate-annual-yearly-zarr-stores-from-hourly-era5-netcdf-files-on-ncars-research-data-archive","position":22},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl3":"Section 5 ( “Generate annual/yearly Zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive”)","lvl2":"Preprocessing Notebooks for NCAR RDA"},"content":"This notebook demonstrates how to preprocess hourly ERA5 NetCDF files from NCAR’s Research Data Archive (RDA) and generate annual/yearly Zarr stores.","type":"content","url":"/#section-5-generate-annual-yearly-zarr-stores-from-hourly-era5-netcdf-files-on-ncars-research-data-archive","position":23},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl3":"Section 6 ( “Calculate Temperature Anomalies”)","lvl2":"Preprocessing Notebooks for NCAR RDA"},"type":"lvl3","url":"/#section-6-calculate-temperature-anomalies","position":24},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl3":"Section 6 ( “Calculate Temperature Anomalies”)","lvl2":"Preprocessing Notebooks for NCAR RDA"},"content":"This notebook demonstrates how to calculate temperature anomalies from the annual 2-m temperature Zarr store generated in Section 5.","type":"content","url":"/#section-6-calculate-temperature-anomalies","position":25},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl2":"Running the Notebooks"},"type":"lvl2","url":"/#running-the-notebooks","position":26},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl2":"Running the Notebooks"},"content":"You can either run the notebook using \n\nBinder or on your local machine.","type":"content","url":"/#running-the-notebooks","position":27},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-binder","position":28},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl3":"Running on Binder","lvl2":"Running the Notebooks"},"content":"The simplest way to interact with a Jupyter Notebook is through\n\n\nBinder, which enables the execution of a\n\n\nJupyter Book in the cloud. The details of how this works are not\nimportant for now. All you need to know is how to launch a Pythia\nCookbooks chapter via Binder. Simply navigate your mouse to\nthe top right corner of the book chapter you are viewing and click\non the rocket ship icon, (see figure below), and be sure to select\n“launch Binder”. After a moment you should be presented with a\nnotebook that you can interact with. I.e. you’ll be able to execute\nand even change the example programs. You’ll see that the code cells\nhave no output at first, until you execute them by pressing\nShift+Enter. Complete details on how to interact with\na live Jupyter notebook are described in \n\nGetting Started with\nJupyter.","type":"content","url":"/#running-on-binder","position":29},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"type":"lvl3","url":"/#running-on-your-own-machine","position":30},{"hierarchy":{"lvl1":"ARCO ERA-5 Interactive Visualization Cookbook","lvl3":"Running on Your Own Machine","lvl2":"Running the Notebooks"},"content":"If you are interested in running this material locally on your computer, you will need to follow this workflow:\n\nClone the https://github.com/ProjectPythia/ERA5_interactive-cookbook repository: git clone https://github.com/ProjectPythia/ERA5_interactive-cookbook.git\n\nMove into the ERA5_interactive-cookbook directorycd ERA5_interactive-cookbook\n\nCreate and activate your conda/mamba environment from the environment.yml filemamba env create -f environment.yml\nmamba activate ERA5_interactive\n\nMove into the notebooks directory and start up Jupyterlabcd notebooks/\njupyter lab","type":"content","url":"/#running-on-your-own-machine","position":31},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy"},"type":"lvl1","url":"/notebooks/basicvisualization","position":0},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy"},"content":"  \n\n","type":"content","url":"/notebooks/basicvisualization","position":1},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy"},"type":"lvl1","url":"/notebooks/basicvisualization#basic-visualization-using-matplotlib-and-cartopy","position":2},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy"},"content":"\n\n\n\n","type":"content","url":"/notebooks/basicvisualization#basic-visualization-using-matplotlib-and-cartopy","position":3},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/basicvisualization#overview","position":4},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy","lvl2":"Overview"},"content":"A team at \n\nGoogle Research & Cloud are making parts of the \n\nECMWF Reanalysis version 5 (aka ERA-5) accessible in a \n\nAnalysis Ready, Cloud Optimized (aka ARCO) format.\n\nIn this notebook, we will do the following:\n\nAccess the \n\nERA-5 ARCO catalog\n\nSelect a particular dataset and variable from the catalog\n\nConvert the data from Gaussian to Cartesian coordinates\n\nPlot a map of sea-level pressure at a specific date and time using Matplotlib and Cartopy.\n\n","type":"content","url":"/notebooks/basicvisualization#overview","position":5},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/basicvisualization#prerequisites","position":6},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nCartopy\n\nNecessary\n\n\n\nXarray\n\nNecessary\n\n\n\nTime to learn: 30 minutes\n\n\n\n","type":"content","url":"/notebooks/basicvisualization#prerequisites","position":7},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/basicvisualization#imports","position":8},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy","lvl2":"Imports"},"content":"\n\nimport fsspec\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport scipy.spatial\nimport numpy as np\nimport cf_xarray as cfxr\n\n","type":"content","url":"/notebooks/basicvisualization#imports","position":9},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy","lvl2":"Access the ARCO ERA-5 catalog on Google Cloud"},"type":"lvl2","url":"/notebooks/basicvisualization#access-the-arco-era-5-catalog-on-google-cloud","position":10},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy","lvl2":"Access the ARCO ERA-5 catalog on Google Cloud"},"content":"\n\nTest bucket access with fsspec\n\nfs = fsspec.filesystem('gs')\nfs.ls('gs://gcp-public-data-arco-era5/co')\n\nLet’s open the single-level-reanalysis Zarr file.\n\nreanalysis = xr.open_zarr(\n    'gs://gcp-public-data-arco-era5/co/single-level-reanalysis.zarr', \n    chunks={'time': 48},\n    consolidated=True,\n)\n\nprint(f'size: {reanalysis.nbytes / (1024 ** 4)} TB')\n\nThat’s ... a big file! But Xarray is just lazy loading the data. We can access the dataset’s metadata:\n\nreanalysis\n\nLet’s look at the mean sea-level pressure variable.\n\nmsl = reanalysis.msl\nmsl\n\nThere are two dimensions to this variable ... time and values. The former is straightforward:\n\nmsl.time\n\nThe time resolution is hourly, commencing at 0000 UTC 1 January 1979 and running through 2300 UTC 31 August 2021.\n\nThe second dimension, values, represents the actual data values. In order to usefully visualize and/or analyze it, we will need to regrid it onto a standard cartesian (in this case, latitude-longitude) grid.\n\nDanger!It might be tempting to run the code cell msl.values here, but doing so will force all the data to be actively read into memory! Since this is a very large dataset, we definitely don't want to do that!\n\n","type":"content","url":"/notebooks/basicvisualization#access-the-arco-era-5-catalog-on-google-cloud","position":11},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy","lvl2":"Regrid to cartesian coordinates"},"type":"lvl2","url":"/notebooks/basicvisualization#regrid-to-cartesian-coordinates","position":12},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy","lvl2":"Regrid to cartesian coordinates"},"content":"\n\nThese reanalyses are in their native, Guassian coordinates. We will define and use several functions to convert them to a lat-lon grid, using several functions described in the \n\nARCO ERA-5 GCP example notebooks\n\ndef mirror_point_at_360(ds):\n  extra_point = (\n      ds.where(ds.longitude == 0, drop=True)\n      .assign_coords(longitude=lambda x: x.longitude + 360)\n  )\n  return xr.concat([ds, extra_point], dim='values')\n\ndef build_triangulation(x, y):\n  grid = np.stack([x, y], axis=1)\n  return scipy.spatial.Delaunay(grid)\n\ndef interpolate(data, tri, mesh):\n  indices = tri.find_simplex(mesh)\n  ndim = tri.transform.shape[-1]\n  T_inv = tri.transform[indices, :ndim, :]\n  r = tri.transform[indices, ndim, :]\n  c = np.einsum('...ij,...j', T_inv, mesh - r)\n  c = np.concatenate([c, 1 - c.sum(axis=-1, keepdims=True)], axis=-1)\n  result = np.einsum('...i,...i', data[:, tri.simplices[indices]], c)\n  return np.where(indices == -1, np.nan, result)\n\nSelect a particular time range from the dataset\n\nds93 = msl.sel(time=slice('1993-03-13T18:00:00','1993-03-13T19:00:00')).compute().pipe(mirror_point_at_360)\nds93\n\nRegrid to a lat-lon grid.\n\ntri = build_triangulation(ds93.longitude, ds93.latitude)\n\nlongitude = np.linspace(0, 360, num=360*4+1)\nlatitude = np.linspace(-90, 90, num=180*4+1)\n\nmesh = np.stack(np.meshgrid(longitude, latitude, indexing='ij'), axis=-1)\n\ngrid_mesh = interpolate(ds93.values, tri, mesh)\n\nConstruct an Xarray DataArray from the regridded array.\n\nda = xr.DataArray(data=grid_mesh,\n                dims=[\"time\", \"longitude\", \"latitude\"],\n                coords=[('time', ds93.time.data), ('longitude', longitude), ('latitude', latitude)],\n                name='msl')\n\nAdd some metadata to the DataArray’s coordinate variables.\n\nda.longitude.attrs['long_name'] = 'longitude'\nda.longitude.attrs['short_name'] = 'lon'\nda.longitude.attrs['units'] = 'degrees_east'\nda.longitude.attrs['axis'] = 'X'\n\nda.latitude.attrs['long_name'] = 'latitude'\nda.latitude.attrs['short_name'] = 'lat'\nda.latitude.attrs['units'] = 'degrees_north'\nda.latitude.attrs['axis'] = 'Y'\n\nda.time.attrs['long_name'] = 'time'\n\nda\n\nSelect only the first time in the DataArray and convert to hPa.\n\nslp = da.isel(time=0)/100.\n\nGet a quick look at the grid to ensure all looks good.\n\nslp.plot(x='longitude', y='latitude', cmap='viridis', size=7, aspect=2, add_colorbar=True, robust=True)\n\n","type":"content","url":"/notebooks/basicvisualization#regrid-to-cartesian-coordinates","position":13},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy","lvl2":"Plot the data on a map"},"type":"lvl2","url":"/notebooks/basicvisualization#plot-the-data-on-a-map","position":14},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy","lvl2":"Plot the data on a map"},"content":"\n\nnx,ny = np.meshgrid(longitude,latitude,indexing='ij')\n\ntimeStr = '1993-03-13T18:00'\n\ntl1 = f'ERA-5 SLP (hPa)'\ntl2 = f'Valid at: {timeStr}'\ntitle_line = (tl1 + '\\n' + tl2 + '\\n') # concatenate the two strings and add return characters\n\ncint = np.arange(900,1080,4)\n\nres = '50m'\nfig = plt.figure(figsize=(15,10))\nax = plt.subplot(1,1,1,projection=ccrs.PlateCarree())\n#ax.set_extent ([lonW+constrainLon,lonE-constrainLon,latS+constrainLat,latN-constrainLat])\nax.add_feature(cfeature.COASTLINE.with_scale(res))\nax.add_feature(cfeature.STATES.with_scale(res))\n#CL = slp.cf.plot.contour(levels=cint,linewidths=1.25,colors='green')\nCL = ax.contour(nx, ny, slp, transform=ccrs.PlateCarree(),levels=cint, linewidths=1.25, colors='green')\nax.clabel(CL, inline_spacing=0.2, fontsize=11, fmt='%.0f')\n\ntitle = plt.title(title_line,fontsize=16)\n\n\n\n\n","type":"content","url":"/notebooks/basicvisualization#plot-the-data-on-a-map","position":15},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/basicvisualization#summary","position":16},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy","lvl2":"Summary"},"content":"In this notebook, we have accessed one of the ARCO ERA-5 datasets, regridded from the ECMWF native spectral to cartesian lat-lon coordinates, and created a map of sea-level pressure for a particular date and time.","type":"content","url":"/notebooks/basicvisualization#summary","position":17},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy","lvl3":"What’s next?","lvl2":"Summary"},"type":"lvl3","url":"/notebooks/basicvisualization#whats-next","position":18},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy","lvl3":"What’s next?","lvl2":"Summary"},"content":"In the next notebook, we will leverage the \n\nHoloviz ecosystem and create interactive visualizations of the ARCO ERA-5 datasets.\n\n","type":"content","url":"/notebooks/basicvisualization#whats-next","position":19},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/basicvisualization#resources-and-references","position":20},{"hierarchy":{"lvl1":"Basic Visualization using matplotlib and Cartopy","lvl2":"Resources and references"},"content":"This notebook follows the general workflow as used in the \n\nGoogle Research ARCO-ERA5 Surface Reanlysis Walkthrough notebook","type":"content","url":"/notebooks/basicvisualization#resources-and-references","position":21},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews"},"type":"lvl1","url":"/notebooks/interactivevisualization","position":0},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews"},"content":"  \n\n","type":"content","url":"/notebooks/interactivevisualization","position":1},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews"},"type":"lvl1","url":"/notebooks/interactivevisualization#interactive-visualization-using-geoviews","position":2},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews"},"content":"\n\n\n\n","type":"content","url":"/notebooks/interactivevisualization#interactive-visualization-using-geoviews","position":3},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/interactivevisualization#overview","position":4},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl2":"Overview"},"content":"A team at \n\nGoogle Research & Cloud are making parts of the \n\nECMWF Reanalysis version 5 (aka ERA-5) accessible in a \n\nAnalysis Ready, Cloud Optimized (aka ARCO) format.\n\nIn this notebook, we will do the following:\n\nAccess the \n\nERA-5 ARCO catalog\n\nSelect a particular dataset and variable from the catalog\n\nConvert the data from Gaussian to Cartesian coordinates\n\nPlot a map of sea-level pressure contours and 2-meter temperature mesh using Geoviews.\n\n","type":"content","url":"/notebooks/interactivevisualization#overview","position":5},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/interactivevisualization#prerequisites","position":6},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nCartopy\n\nNecessary\n\n\n\nXarray\n\nNecessary\n\n\n\n[Geoviews]\n\nNecessary\n\n\n\nTime to learn: 30 minutes\n\n\n\n","type":"content","url":"/notebooks/interactivevisualization#prerequisites","position":7},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/interactivevisualization#imports","position":8},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl2":"Imports"},"content":"\n\nimport fsspec\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport scipy.spatial\nimport numpy as np\nimport geoviews as gv\nfrom geoviews import opts\nfrom pyproj import Transformer\nfrom holoviews.operation.datashader import rasterize\n\n","type":"content","url":"/notebooks/interactivevisualization#imports","position":9},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl2":"Access the ARCO ERA-5 catalog on Google Cloud"},"type":"lvl2","url":"/notebooks/interactivevisualization#access-the-arco-era-5-catalog-on-google-cloud","position":10},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl2":"Access the ARCO ERA-5 catalog on Google Cloud"},"content":"\n\nLet’s open the single-level-reanalysis Zarr file and save two variables from it\n\nreanalysis = xr.open_zarr(\n    'gs://gcp-public-data-arco-era5/co/single-level-reanalysis.zarr', \n    chunks={'time': 48},\n    consolidated=True,\n)\n\nmsl = reanalysis.msl\nt2m = reanalysis.t2m\n\n","type":"content","url":"/notebooks/interactivevisualization#access-the-arco-era-5-catalog-on-google-cloud","position":11},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl2":"Regrid to Cartesian coordinates"},"type":"lvl2","url":"/notebooks/interactivevisualization#regrid-to-cartesian-coordinates","position":12},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl2":"Regrid to Cartesian coordinates"},"content":"\n\nThese reanalyses are in their native, Guassian coordinates. We will define and use several functions to convert them to a lat-lon grid, as documented in the \n\nARCO ERA-5 GCP example notebooks\n\ndef mirror_point_at_360(ds):\n  extra_point = (\n      ds.where(ds.longitude == 0, drop=True)\n      .assign_coords(longitude=lambda x: x.longitude + 360)\n  )\n  return xr.concat([ds, extra_point], dim='values')\n\ndef build_triangulation(x, y):\n  grid = np.stack([x, y], axis=1)\n  return scipy.spatial.Delaunay(grid)\n\ndef interpolate(data, tri, mesh):\n  indices = tri.find_simplex(mesh)\n  ndim = tri.transform.shape[-1]\n  T_inv = tri.transform[indices, :ndim, :]\n  r = tri.transform[indices, ndim, :]\n  c = np.einsum('...ij,...j', T_inv, mesh - r)\n  c = np.concatenate([c, 1 - c.sum(axis=-1, keepdims=True)], axis=-1)\n  result = np.einsum('...i,...i', data[:, tri.simplices[indices]], c)\n  return np.where(indices == -1, np.nan, result)\n\nSelect a particular time range from the dataset.\n\nmsl93 = msl.sel(time=slice('1993-03-13T18:00:00','1993-03-14T00:00:00')).compute().pipe(mirror_point_at_360)\nt2m93 = t2m.sel(time=slice('1993-03-13T18:00:00','1993-03-14T00:00:00')).compute().pipe(mirror_point_at_360)\n\nRegrid to a lat-lon grid.\n\ntri = build_triangulation(msl93.longitude, msl93.latitude)\n\nlongitude = np.linspace(0, 360, num=360*4+1)\nlatitude = np.linspace(-90, 90, num=180*4+1)\n\nmesh = np.stack(np.meshgrid(longitude, latitude, indexing='ij'), axis=-1)\n\ngrid_mesh_msl = interpolate(msl93.values, tri, mesh)\ngrid_mesh_t2m = interpolate(t2m93.values, tri, mesh)\n\nConstruct an Xarray DataArray from the regridded array.\n\nda_msl = xr.DataArray(data=grid_mesh_msl,\n                dims=[\"time\", \"longitude\", \"latitude\"],\n                coords=[('time', msl93.time.data), ('longitude', longitude), ('latitude', latitude)],\n                name='msl')\n\nda_t2m = xr.DataArray(data=grid_mesh_t2m,\n                dims=[\"time\", \"longitude\", \"latitude\"],\n                coords=[('time', t2m93.time.data), ('longitude', longitude), ('latitude', latitude)],\n                name='t2m')\n\nConvert to hPa and deg C.\n\nslp = da_msl/100\nt2m = da_t2m-273.15\n\n","type":"content","url":"/notebooks/interactivevisualization#regrid-to-cartesian-coordinates","position":13},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl2":"Plot the data using geoviews"},"type":"lvl2","url":"/notebooks/interactivevisualization#plot-the-data-using-geoviews","position":14},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl2":"Plot the data using geoviews"},"content":"\n\nAdd an interactive element by using the \n\nholoviz/\n\ngeoviews libraries.\n\nWe need to choose the rendering backend that we want to use in Geoviews, in this case we are using Bokeh.\n\n\n\ngv.extension('bokeh')\n\nIf you want the plot restricted to a specific part of the world, you can specify where, however, we need to transform the coordinates from PlateCarree to WebMercator (aka EPSG-3857), which is the default projection for Geoviews with the Bokeh backend.\n\nlonW, lonE, latS, latN = -130, -60, 20, 55 \n\ntransformer = Transformer.from_crs(ccrs.PlateCarree(), \"EPSG:3857\")\nlon1, lat1 = transformer.transform(lonW,latS)\nlon2, lat2 = transformer.transform(lonE, latN)\n\nConvert our Xarray datasets/data arrays into Geoviews dataset objects, specifying the different dimensions of the dataset (lat, lon, time) as well as the variable we want to plot. For this first example, we’ll just visualize the first time in the selected time range.\n\nNote:By default, Geoviews expects a dataset whose coordinates are lat-lon (i.e., using a PlateCarree projection) to have the longitude dimension vary from -180 to 180. In this case, the ERA-5's longitudes range from 0 to 360. In this case, ensure that the crs of the element declares the actual central_longitude, e.g. 180 degrees rather than the default 0 degrees.\n\ndataset_era = gv.Dataset(slp.isel(time=0), kdims=['longitude','latitude'],vdims='msl') #create a geoviews dataset\ncontour = gv.project(dataset_era.to(gv.LineContours, ['longitude', 'latitude'],crs=ccrs.PlateCarree(central_longitude=180))) #create a Geoviews contour object\n\n","type":"content","url":"/notebooks/interactivevisualization#plot-the-data-using-geoviews","position":15},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl3":"Plotting MSLP","lvl2":"Plot the data using geoviews"},"type":"lvl3","url":"/notebooks/interactivevisualization#plotting-mslp","position":16},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl3":"Plotting MSLP","lvl2":"Plot the data using geoviews"},"content":"\n\ncint = np.arange(900,1080,4)\n(gv.tile_sources.OSM * contour).opts(\n    opts.LineContours(tools=['hover'], levels=cint, frame_width=700, frame_height=400,show_legend=False, line_width=3,xlim=(lon1,lon2),ylim=(lat1,lat2)))\n\nUse the Bokeh tools to the right of the plot to zoom in/out or reset to the original dimensions. The bottom-most tool tip (the hover tool) will produce a pop-up window that shows the sea-level pressure value at the closest grid point to where your cursor lies.\n\nInfoNotice how this only plots a single time. We can iterate over the time dimension by specifying time as a dimension when we create the Geoviews dataset, as shown below!\n\ndataset_era = gv.Dataset(slp, kdims=['longitude','latitude','time'],vdims='msl') #create a geoviews dataset; here we do not select just a single time\ncontour = gv.project(dataset_era.to(gv.LineContours, ['longitude', 'latitude'])) #create line contours\n\nCreate the interactive visualization. We overlay the SLP contours on a map from a web tile server. We specify various options, such as frame size, spatial extent, and line thickness as well.\n\n(gv.tile_sources.OSM * contour).opts(\n    opts.LineContours(tools=['hover'], levels=cint, frame_width=700, frame_height=400,show_legend=False, line_width=3,xlim=(lon1,lon2),ylim=(lat1,lat2)))\n\nSince there are multiple times in the dataset, we now get a time slider that you can manipulate. Notice that as you change the time, the plot automatically updates!\n\nNoteDid you notice that it took a little longer for the graphic to appear? That is because we have loaded seven total time steps, instead of just one.\n\n","type":"content","url":"/notebooks/interactivevisualization#plotting-mslp","position":17},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl3":"Plotting T2M","lvl2":"Plot the data using geoviews"},"type":"lvl3","url":"/notebooks/interactivevisualization#plotting-t2m","position":18},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl3":"Plotting T2M","lvl2":"Plot the data using geoviews"},"content":"\n\nNow let’s plot 2 meter temperatures. This time, we’ll visualize the data via a Quadmesh (gv.Quadmesh) instead of using contour lines (gv.LineContours).\n\nQuadmesh is more computationally expensive, so it is good practice to subset the data from its original global extent to an area that you’re interested in. Here, we select the same region as we did earlier for sea-level pressure.\n\ndef lon_to_360(dlon: float) -> float:\n    return ((360 + (dlon % 360)) % 360)\n\ncond = (t2m.longitude > lon_to_360(lonW)) & (t2m.latitude > latS) & \\\n       (t2m.longitude < lon_to_360(lonE)) & (t2m.latitude < latN)\n\nt2m_cropped = t2m.where(cond, drop=True)\n\ndataset_era = gv.Dataset(t2m_cropped, kdims=['longitude','latitude','time'],vdims='t2m')\nqm = gv.project(dataset_era.to(gv.QuadMesh, ['longitude', 'latitude']))\n\n\n(gv.tile_sources.OSM().opts(xlim=(lon1, lon2),ylim=(lat1, lat2),title=f'ERA-5 2mT', frame_width=700, frame_height=400)  * (qm.opts(tools=['hover'], axiswise=True, cmap='coolwarm', colorbar=True, clim=(-50,50), alpha=0.8)))\n\nYou probably noticed this visualization took a while to render. A more performant strategy is to rasterize the Geoviews dataset.\n\nWarningIf you plot a large dataset without rasterizing it, e.g. the entire globe for multiple timeplots of data, load times and RAM will scale up in tandem.\n\nqm_raster = rasterize(qm, precompute=True)\n\n(gv.tile_sources.OSM().opts(xlim=(lon1, lon2),ylim=(lat1, lat2),title=f'ERA-5 2mT', frame_width=700, frame_height=400)  * (qm_raster.opts(tools=['hover'], axiswise=True, cmap='coolwarm', colorbar=True, clim=(-50,50), alpha=0.8)))\n\nTime slider bug!While the visualization loaded much faster, the rasterized dataset does not update as we manipulate the time slider!\n\n","type":"content","url":"/notebooks/interactivevisualization#plotting-t2m","position":19},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl2":"What’s next?"},"type":"lvl2","url":"/notebooks/interactivevisualization#whats-next","position":20},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl2":"What’s next?"},"content":"In the next notebook, we will add more user-configurable options to our Geoviews-powered visualizations, using another key player in the Holoviz ecosystem, \n\nPanel.\n\n","type":"content","url":"/notebooks/interactivevisualization#whats-next","position":21},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/interactivevisualization#summary","position":22},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl2":"Summary"},"content":"In this notebook, we have accessed one of the ARCO ERA-5 datasets, regridded from the ECMWF native spectral to cartesian lat-lon coordinates, and used geoviews to create interactive maps of sea-level pressure and 2-meter temperature.\n\n","type":"content","url":"/notebooks/interactivevisualization#summary","position":23},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl2":"Resources and references"},"type":"lvl2","url":"/notebooks/interactivevisualization#resources-and-references","position":24},{"hierarchy":{"lvl1":"Interactive Visualization using GeoViews","lvl2":"Resources and references"},"content":"This notebook follows the general workflow as used in the \n\nGoogle Research ARCO-ERA5 Surface Reanlysis Walkthrough notebook\n\nHoloviz\n\nGeoviews","type":"content","url":"/notebooks/interactivevisualization#resources-and-references","position":25},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot"},"type":"lvl1","url":"/notebooks/hvplot","position":0},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot"},"content":"","type":"content","url":"/notebooks/hvplot","position":1},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot"},"type":"lvl1","url":"/notebooks/hvplot#interactive-visualuzation-using-hvplot","position":2},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot"},"content":"\n\n\n\n","type":"content","url":"/notebooks/hvplot#interactive-visualuzation-using-hvplot","position":3},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/hvplot#overview","position":4},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot","lvl2":"Overview"},"content":"ERA-5 Dataset is available from NCAR RDA in netcdf format. A subset of this dataset is processed into Zarr format and available from NCAR RDA endpoints. To learn how you can create Zarr files from NCAR RDA netcdf files, please see \n\nthis notebook.\n\nBy the end of this notebook, you should be able to:\n\nUnderstand the importance for interactive plots and the challenges associated with them\n\nUse hvPlot to generate basic interactive plots with Xarray\n\n","type":"content","url":"/notebooks/hvplot#overview","position":5},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/hvplot#prerequisites","position":6},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Xarray\n\nNecessary\n\n\n\nTime to learn: 30 minutes\n\n","type":"content","url":"/notebooks/hvplot#prerequisites","position":7},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/hvplot#imports","position":8},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot","lvl2":"Imports"},"content":"\n\nimport holoviews as hv\nimport xarray as xr\nfrom holoviews import opts\n\nhv.extension(\"bokeh\")\n\n","type":"content","url":"/notebooks/hvplot#imports","position":9},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot","lvl2":"Data"},"type":"lvl2","url":"/notebooks/hvplot#data","position":10},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot","lvl2":"Data"},"content":"As we mentioned above a subset of NCAR RDA data is available in Zarr format.\n\nrda_url = \"https://data.rda.ucar.edu/\"\nannual_means = rda_url + \"pythia_era5_24/annual_means/\"\nxrds = xr.open_dataset(annual_means + \"temp_2m_annual_1940_2023.zarr\", engine=\"zarr\")\nxrds = xrds.isel(time=slice(0, 5))\nxrds.load()\n\n","type":"content","url":"/notebooks/hvplot#data","position":11},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot","lvl2":"Considerations for Interactive Plots"},"type":"lvl2","url":"/notebooks/hvplot#considerations-for-interactive-plots","position":12},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot","lvl2":"Considerations for Interactive Plots"},"content":"Add some markdown text on some of the following ideas:\n\nWhat are some reasons we want to make data visualuzation interactive?\n\n","type":"content","url":"/notebooks/hvplot#considerations-for-interactive-plots","position":13},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot","lvl2":"Baisc Interactivity using hvPlot"},"type":"lvl2","url":"/notebooks/hvplot#baisc-interactivity-using-hvplot","position":14},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot","lvl2":"Baisc Interactivity using hvPlot"},"content":"The hvPlot package is a familiar and high level API for data exploration and visualuzation.\n\n\n\nOne of the most powerfull features of hvPlot is that it provides an alternative plotting API that directly attaches to existing Python objects through the .hvplot() attribute. For the case of Xarray, importing hvplot.xarray adds a brand new set of plotting routines accessible either through xr.DataArray.hvplot() or xr.Dataset.hvplot()\n\nimport hvplot.xarray\n\nBefore using hvPlot, let’s take a look at the default Xarray plotting methods.\n\nxrds[\"VAR_2T\"].plot()\n\nWe can replace the .plot() function call with .hvplot(). By default, hvPlot uses the Bokeh backend, which has naitive interactive tools, such as :\n\nPanning\n\nBox Select\n\nScroll Zoom\n\nSaving\n\nResetting\n\nxrds[\"VAR_2T\"].hvplot()\n\nIf we wanted to plot ...\n\nxrds[\"VAR_2T\"].isel(time=0).plot()\n\nSwitching\n\nxrds[\"VAR_2T\"].isel(time=0).hvplot()\n\n","type":"content","url":"/notebooks/hvplot#baisc-interactivity-using-hvplot","position":15},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot","lvl3":"Time Widget","lvl2":"Baisc Interactivity using hvPlot"},"type":"lvl3","url":"/notebooks/hvplot#time-widget","position":16},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot","lvl3":"Time Widget","lvl2":"Baisc Interactivity using hvPlot"},"content":"\n\nClimate data typically comes with multiple timesteps. We can create a basic widget that allows us to seek through time by setting the groupby='time' parameter in our .hvplot() call.\n\nxrds[\"VAR_2T\"].hvplot(groupby=\"time\", widget_location=\"bottom\")\n\nYou may notice that our colorbar is dynamically changing as we change our time steps. We can fix the colorbar by setting a clim value, which is a tuple of the minimum and maximum desired colorbar range.\n\nOne suggestion is to use the minimum and maximum of the data variable you are visualuzing across time.\n\nclim = (xrds[\"VAR_2T\"].values.min(), xrds[\"VAR_2T\"].values.max())\n\nxrds[\"VAR_2T\"].hvplot(clim=clim, groupby=\"time\", widget_location=\"bottom\")\n\nYou may have noticed that there is a slight lag when switching time steps. This is due to hvPlot plotting the full resolution of our dataset. We can instead rasterize the output by setting rasterize=True, which will significantly improve the perfromance of our interactive plot.\n\nxrds[\"VAR_2T\"].hvplot(\n    rasterize=True, clim=clim, groupby=\"time\", widget_location=\"bottom\"\n)\n\n","type":"content","url":"/notebooks/hvplot#time-widget","position":17},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot","lvl3":"Animation Widget","lvl2":"Baisc Interactivity using hvPlot"},"type":"lvl3","url":"/notebooks/hvplot#animation-widget","position":18},{"hierarchy":{"lvl1":"Interactive Visualuzation using hvPlot","lvl3":"Animation Widget","lvl2":"Baisc Interactivity using hvPlot"},"content":"\n\nAnother usefull interactive feature is animations. Instead of manually scrolling through time, we can set up a widget that lets us animate our data across time. This can be achieved by adding a Scrubber widget to our plot by setting widget_type=\"scrubber\"\n\nxrds[\"VAR_2T\"].hvplot(\n    rasterize=True,\n    groupby=\"time\",\n    widget_type=\"scrubber\",\n    widget_location=\"bottom\",\n)","type":"content","url":"/notebooks/hvplot#animation-widget","position":19},{"hierarchy":{"lvl1":"Creating an Interactive Dashboard using hvPlot and Panel"},"type":"lvl1","url":"/notebooks/dashboard","position":0},{"hierarchy":{"lvl1":"Creating an Interactive Dashboard using hvPlot and Panel"},"content":"\n\nimport xarray as xr\nimport holoviews as hv\nimport panel as pn\nimport hvplot.xarray\nfrom holoviews import opts\n\npn.extension()\n\n\n","type":"content","url":"/notebooks/dashboard","position":1},{"hierarchy":{"lvl1":"Creating an Interactive Dashboard using hvPlot and Panel","lvl2":"Data"},"type":"lvl2","url":"/notebooks/dashboard#data","position":2},{"hierarchy":{"lvl1":"Creating an Interactive Dashboard using hvPlot and Panel","lvl2":"Data"},"content":"\n\nIn this notebook, we are going to load annual mean dataset of 2-m temperature from the \n\nERA5 reanalysis that we preprocessed in Zarr format. Please see the preprocessing notebooks for the required steps.\n\n\n\nrda_url = \"https://data.rda.ucar.edu/\"\nannual_means = rda_url + \"pythia_era5_24/annual_means/\"\nxrds = xr.open_dataset(annual_means + \"temp_2m_annual_1940_2023.zarr\", engine=\"zarr\")\nxrds\n\n# Select the time range of interest\nxrds = xrds.sel(time=slice('2017-01-01', '2023-12-31'))\nxrds.load();\n\n","type":"content","url":"/notebooks/dashboard#data","position":3},{"hierarchy":{"lvl1":"Creating an Interactive Dashboard using hvPlot and Panel","lvl2":"Panel Widgets"},"type":"lvl2","url":"/notebooks/dashboard#panel-widgets","position":4},{"hierarchy":{"lvl1":"Creating an Interactive Dashboard using hvPlot and Panel","lvl2":"Panel Widgets"},"content":"Panel provides a variety of widgets that can be used to build interactive dashboards. In this notebook, we are going to use some of these widgets. For the complete list of widgets, please see the \n\nPanel documentation.\n\nThe panel widgets that we are using are:\n\npn.widgets.Select for selecting the variable\n\npn.widgets.DatePicker for selecting the date\n\npn.widgets.Player for making time series animations\n\n\nw_var = pn.widgets.Select(name=\"Data Variable\", options=list(xrds.data_vars))\n\ndataset_controls = pn.WidgetBox(\n    \"## Dataset Controls\",\n    w_var,\n)\ndataset_controls\n\nNow, let’s create dropdown widgets for selecting the colormap and plot type.\n\nw_cmap = pn.widgets.Select(name=\"Colormap\", options=[\"inferno\", \"plasma\", \"coolwarm\"])\n\n\nw_plot_type = pn.widgets.Select(\n    name=\"Plot Type\", options=[\"Color Plot\", \"Contour\"]\n)\n\n\nplot_controls = pn.WidgetBox(\n    \"## Plot Controls\",\n    w_plot_type,\n    w_cmap,\n)\nplot_controls\n\nNow, let’s put together all the controls and the plot in a panel layout using pn.Row and pn.Column:\n\ncontrols = pn.Column(dataset_controls, plot_controls)\ncontrols\n\nw_player = pn.widgets.Player(\n    value=0,\n    start=0,\n    end=len(xrds.time) - 1,\n    name=\"Year\",\n    loop_policy=\"loop\",\n    interval=300,\n    align=\"center\",\n    width_policy=\"fit\",\n)\nw_player\n\n","type":"content","url":"/notebooks/dashboard#panel-widgets","position":5},{"hierarchy":{"lvl1":"Creating an Interactive Dashboard using hvPlot and Panel","lvl2":"Plotting Function"},"type":"lvl2","url":"/notebooks/dashboard#plotting-function","position":6},{"hierarchy":{"lvl1":"Creating an Interactive Dashboard using hvPlot and Panel","lvl2":"Plotting Function"},"content":"\n\ndef plot_ds(time, var, cmap, plot_type):\n    clim = (xrds[var].values.min(), xrds[var].values.max())\n\n    if plot_type == \"Color Plot\":\n        return (\n            xrds[var]\n            .isel(time=time)\n            .hvplot(\n                cmap=cmap,\n                title=str(f\"{var} year {time}\"),\n                clim=clim,\n                dynamic=False,\n                rasterize=True,\n                precompute=True,\n            )\n            .opts(framewise=False)\n        )\n\n    elif plot_type == \"Contour\":\n        return (\n            xrds[var]\n            .isel(time=time)\n            .hvplot.contour(\n                cmap=cmap,\n                dynamic=False,\n                rasterize=True,\n                title=str(f\"{var} Year: {time}\"),\n                clim=clim,\n                precompute=True,\n            )\n            .opts(framewise=False)\n        )\n\n\napp = pn.Row(\n    controls,\n    pn.Column(\n        pn.panel(\n            hv.DynamicMap(\n                pn.bind(\n                    plot_ds,\n                    time=w_player,\n                    var=w_var,\n                    cmap=w_cmap,\n                    plot_type=w_plot_type,\n                )\n            )\n        ),\n        w_player,\n    ),\n).servable()\n\napp\n\nPlease note how the above dashboard is servable. You can deploy the dashboard by running the following command in the terminal:panel serve --show 04_dashboard.ipynb --allow-websocket-origin=projectpythia.2i2c.cloud\n\nThis will open a new tab in your default web browser with the dashboard. The allow websocket origin flag is required to allow traffic to flow to the site. This should be update to reflect the base URL where the application is launched. A wildcard can be used, *, to allow traffic from any site to connect.\n\nOn the Project Pythia 2i2c hosted JupyterHub the link to use to access the panel serve command is \n\nhttps://​projectpythia​.2i2c​.cloud​/hub​/user​-redirect​/proxy​/5006​/04​_dashboard","type":"content","url":"/notebooks/dashboard#plotting-function","position":7},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive"},"type":"lvl1","url":"/notebooks/data-preprocessing","position":0},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive"},"content":"","type":"content","url":"/notebooks/data-preprocessing","position":1},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl2":"Overview and Warning: Please Read"},"type":"lvl2","url":"/notebooks/data-preprocessing#overview-and-warning-please-read","position":2},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl2":"Overview and Warning: Please Read"},"content":"ERA5 data on NCAR is stored in hourly NetCDF files. Therefore, it is necessary to create intermediate ARCO datasets for fast processing.\n\nIn this notebook, we read hourly data from NCAR’s publicly accessible ERA5 collection using an intake catalog, compute the annual means and store the result using zarr stores.\n\nIf you don’t have write permision to save to the Research Data Archive (RDA), please save the result to your local folder.\n\nIf you need annual means for the following variables, please don’t run this notebook. The data has already been calculated and can be accessed via https from \n\nhttps://​data​.rda​.ucar​.edu​/pythia​_era5​_24​/annual​_means/\n\nAir temperature at 2 m/ VAR_2T (\n\nhttps://​data​.rda​.ucar​.edu​/pythia​_era5​_24​/annual​_means​/temp​_2m​_annual​_1940​_2023​.zarr)\n\nOtherwise, please run this script once to generate the annual means.\n\n","type":"content","url":"/notebooks/data-preprocessing#overview-and-warning-please-read","position":3},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/data-preprocessing#prerequisites","position":4},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Xarray\n\nNecessary\n\n\n\nIntro to Intake\n\nNecessary\n\n\n\nUnderstanding of Zarr\n\nHelpful\n\n\n\nTime to learn: 30 minutes\n\n","type":"content","url":"/notebooks/data-preprocessing#prerequisites","position":5},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/data-preprocessing#imports","position":6},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl2":"Imports"},"content":"\n\nimport glob\nimport re\nimport matplotlib as plt\nimport numpy as np\nimport scipy as sp\nimport xarray as xr\nimport intake\nimport intake_esm\nimport pandas as pd\n\nimport dask\nfrom dask.distributed import Client, performance_report\nfrom dask_jobqueue import PBSCluster\n\n######## File paths ################\nrda_scratch       = \"/gpfs/csfs1/collections/rda/scratch/harshah\"\nrda_data          = \"/gpfs/csfs1/collections/rda/data/\"\n#########\nrda_url           = 'https://data.rda.ucar.edu/'\nera5_catalog      = rda_url + 'pythia_era5_24/pythia_intake_catalogs/era5_catalog.json'\n#alternate_catalog = rda_data + 'pythia_era5_24/pythia_intake_catalogs/era5_catalog_opendap.json'\nannual_means      =  rda_data + 'pythia_era5_24/annual_means/'\n######## \nzarr_path         = rda_scratch + \"/tas_zarr/\"\n##########\nprint(annual_means)\n\n","type":"content","url":"/notebooks/data-preprocessing#imports","position":7},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl2":"Create a Dask cluster"},"type":"lvl2","url":"/notebooks/data-preprocessing#create-a-dask-cluster","position":8},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl2":"Create a Dask cluster"},"content":"\n\n","type":"content","url":"/notebooks/data-preprocessing#create-a-dask-cluster","position":9},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl4":"Dask Introduction","lvl2":"Create a Dask cluster"},"type":"lvl4","url":"/notebooks/data-preprocessing#dask-introduction","position":10},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl4":"Dask Introduction","lvl2":"Create a Dask cluster"},"content":"Dask is a solution that enables the scaling of Python libraries. It mimics popular scientific libraries such as numpy, pandas, and xarray that enables an easier path to parallel processing without having to refactor code.\n\nThere are 3 components to parallel processing with Dask: the client, the scheduler, and the workers.\n\nThe Client is best envisioned as the application that sends information to the Dask cluster. In Python applications this is handled when the client is defined with client = Client(CLUSTER_TYPE). A Dask cluster comprises of a single scheduler that manages the execution of tasks on workers. The CLUSTER_TYPE can be defined in a number of different ways.\n\nThere is LocalCluster, a cluster running on the same hardware as the application and sharing the available resources, directly in Python with dask.distributed.\n\nIn certain JupyterHubs Dask Gateway may be available and a dedicated dask cluster with its own resources can be created dynamically with dask.gateway.\n\nOn HPC systems dask_jobqueue is used to connect to the HPC Slurm and PBS job schedulers to provision resources.\n\nThe dask.distributed client python module can also be used to connect to existing clusters. A Dask Scheduler and Workers can be deployed in containers, or on Kubernetes, without using a Python function to create a dask cluster. The dask.distributed Client is configured to connect to the scheduler either by container name, or by the Kubernetes service name.\n\n","type":"content","url":"/notebooks/data-preprocessing#dask-introduction","position":11},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl4":"Select the Dask cluster type","lvl2":"Create a Dask cluster"},"type":"lvl4","url":"/notebooks/data-preprocessing#select-the-dask-cluster-type","position":12},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl4":"Select the Dask cluster type","lvl2":"Create a Dask cluster"},"content":"\n\nThe default will be LocalCluster as that can run on any system.\n\nIf running on a HPC computer with a PBS Scheduler, set to True. Otherwise, set to False.\n\nUSE_PBS_SCHEDULER = True\n\nIf running on Jupyter server with Dask Gateway configured, set to True. Otherwise, set to False.\n\nUSE_DASK_GATEWAY = False\n\nPython function for a PBS cluster\n\n# Create a PBS cluster object\ndef get_pbs_cluster():\n    \"\"\" Create cluster through dask_jobqueue.   \n    \"\"\"\n    from dask_jobqueue import PBSCluster\n    cluster = PBSCluster(\n        job_name = 'dask-pythia-24',\n        cores = 1,\n        memory = '4GiB',\n        processes = 1,\n        local_directory = rda_scratch + '/dask/spill',\n        resource_spec = 'select=1:ncpus=1:mem=8GB',\n        queue = 'casper',\n        walltime = '1:00:00',\n        #interface = 'ib0'\n        interface = 'ext'\n    )\n    return cluster\n\nPython function for a Gateway Cluster\n\ndef get_gateway_cluster():\n    \"\"\" Create cluster through dask_gateway\n    \"\"\"\n    from dask_gateway import Gateway\n\n    gateway = Gateway()\n    cluster = gateway.new_cluster()\n    cluster.adapt(minimum=2, maximum=4)\n    return cluster\n\nPython function for a Local Cluster\n\ndef get_local_cluster():\n    \"\"\" Create cluster using the Jupyter server's resources\n    \"\"\"\n    from distributed import LocalCluster, performance_report\n    cluster = LocalCluster()    \n\n    cluster.scale(4)\n    return cluster\n\nPython logic to select the Dask Cluster type\n\nThis uses True/False boolean logic based on the variables set in the previous cells\n\n# Obtain dask cluster in one of three ways\n\nif USE_PBS_SCHEDULER:\n    cluster = get_pbs_cluster()\nelif USE_DASK_GATEWAY:\n    cluster = get_gateway_cluster()\nelse:\n    cluster = get_local_cluster()\n\n# Connect to cluster\nfrom distributed import Client\nclient = Client(cluster)\n\n# Display cluster dashboard URL\ncluster\n\n","type":"content","url":"/notebooks/data-preprocessing#select-the-dask-cluster-type","position":13},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl2":"Find data using intake catalog"},"type":"lvl2","url":"/notebooks/data-preprocessing#find-data-using-intake-catalog","position":14},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl2":"Find data using intake catalog"},"content":"\n\nera5_cat = intake.open_esm_datastore(era5_catalog)\nera5_cat\n\nera5_cat.df[['long_name','variable']].drop_duplicates().head()\n\n","type":"content","url":"/notebooks/data-preprocessing#find-data-using-intake-catalog","position":15},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl3":"Select variable of interest","lvl2":"Find data using intake catalog"},"type":"lvl3","url":"/notebooks/data-preprocessing#select-variable-of-interest","position":16},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl3":"Select variable of interest","lvl2":"Find data using intake catalog"},"content":"\n\n######## Examples of other Variables ##############\n# MTNLWRF = Outgoing Long Wave Radiation (upto a sign), Mean Top Net Long Wave Radiative Flux\n# rh_cat = era5_cat.search(variable= 'R') # R =  Relative Humidity\n# olr_cat = era5_cat.search(variable ='MTNLWRF')\n# olr_cat\n############ Access temperature data ###########\ntemp_cat = era5_cat.search(variable='VAR_2T',frequency = 'hourly')\ntemp_cat\n\n# Define the xarray_open_kwargs with a compatible engine, for example, 'scipy'\nxarray_open_kwargs = {\n    'engine': 'h5netcdf',\n    'chunks': {},  # Specify any chunking if needed\n    'backend_kwargs': {}  # Any additional backend arguments if required\n}\n\n%%time\ndset_temp = temp_cat.to_dataset_dict(xarray_open_kwargs=xarray_open_kwargs)\n\ndset_temp.keys()\n\ntemp_2m = dset_temp['an.sfc'].VAR_2T\ntemp_2m\n\ntemp_2m_annual = temp_2m.resample(time='1Y').mean()\ntemp_2m_annual\n\n","type":"content","url":"/notebooks/data-preprocessing#select-variable-of-interest","position":17},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl3":"Save the notbeook","lvl2":"Find data using intake catalog"},"type":"lvl3","url":"/notebooks/data-preprocessing#save-the-notbeook","position":18},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl3":"Save the notbeook","lvl2":"Find data using intake catalog"},"content":"\n\n# temp_2m_annual.to_dataset().to_zarr(zarr_path + \"e5_tas2m_monthly_1940_2023.zarr)\n\ntemp_2m_monthly = xr.open_zarr(zarr_path + \"e5_tas2m_monthly_1940_2023.zarr\").VAR_2T\ntemp_2m_monthly\n\ntemp_2m_annual = temp_2m_monthly.resample(time='1Y').mean()\ntemp_2m_annual = temp_2m_annual.chunk({'latitude':721,'longitude':1440})\ntemp_2m_annual = temp_2m_annual.drop_isel({'time':-1}) # Drop 2024 data\ntemp_2m_annual\n\n","type":"content","url":"/notebooks/data-preprocessing#save-the-notbeook","position":19},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl4":"Save annual mean to annual_means folder within rda_data","lvl3":"Save the notbeook","lvl2":"Find data using intake catalog"},"type":"lvl4","url":"/notebooks/data-preprocessing#save-annual-mean-to-annual-means-folder-within-rda-data","position":20},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl4":"Save annual mean to annual_means folder within rda_data","lvl3":"Save the notbeook","lvl2":"Find data using intake catalog"},"content":"\n\n# %%time\n# temp_2m_annual.to_dataset().to_zarr(annual_means + 'temp_2m_annual_1940_2023.zarr',mode='w')\n\ntemp_2m_annual = xr.open_zarr(annual_means + 'temp_2m_annual_1940_2023.zarr').VAR_2T\ntemp_2m_annual \n\n%%time\ntemp_2m_annual.isel(time=0).plot()\n\n","type":"content","url":"/notebooks/data-preprocessing#save-annual-mean-to-annual-means-folder-within-rda-data","position":21},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl3":"Close up the cluster","lvl2":"Find data using intake catalog"},"type":"lvl3","url":"/notebooks/data-preprocessing#close-up-the-cluster","position":22},{"hierarchy":{"lvl1":"Generate annual/yearly zarr stores from hourly ERA5 NetCDF files on NCAR’s Research Data Archive","lvl3":"Close up the cluster","lvl2":"Find data using intake catalog"},"content":"\n\ncluster.close()","type":"content","url":"/notebooks/data-preprocessing#close-up-the-cluster","position":23},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly"},"type":"lvl1","url":"/notebooks/era5-anomaly","position":0},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly"},"content":"","type":"content","url":"/notebooks/era5-anomaly","position":1},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/era5-anomaly#overview","position":2},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl2":"Overview"},"content":"ERA-5 Dataset is available from NCAR’s RDA in netcdf format as hourly files. A subset of this dataset is processed into Zarr format and available from NCAR RDA endpoints. To learn how you can create Zarr files from NCAR RDA netcdf files, please see \n\nthis notebook.\n\nIn this notebook,\n\nWe will read data zarr stores from NCAR’s RDA endpoint\n\nCompute temperature anomaly for the years 1940-2023\n\n","type":"content","url":"/notebooks/era5-anomaly#overview","position":3},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl2":"Prerequisites"},"type":"lvl2","url":"/notebooks/era5-anomaly#prerequisites","position":4},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl2":"Prerequisites"},"content":"Concepts\n\nImportance\n\nNotes\n\nIntro to Xarray\n\nNecessary\n\n\n\nIntro to Intake\n\nNecessary\n\n\n\nUnderstanding of Zarr\n\nHelpful\n\n\n\nTime to learn: 30 minutes\n\n","type":"content","url":"/notebooks/era5-anomaly#prerequisites","position":5},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl2":"Imports"},"type":"lvl2","url":"/notebooks/era5-anomaly#imports","position":6},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl2":"Imports"},"content":"\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport xarray as xr\nimport intake_esm\nimport intake\nimport pandas as pd\nimport cartopy.crs as ccrs  # Correct import for coordinate reference systems\nimport cartopy.feature as cfeature\nfrom holoviews import opts\nimport geoviews as gv\nimport holoviews as hv\nimport aiohttp\n\n","type":"content","url":"/notebooks/era5-anomaly#imports","position":7},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl3":"Specify global variables","lvl2":"Imports"},"type":"lvl3","url":"/notebooks/era5-anomaly#specify-global-variables","position":8},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl3":"Specify global variables","lvl2":"Imports"},"content":"\n\nbaseline_year_start = 1940\nbaseline_year_end   = 1949\ncurrent_year  = 2023\n\nrda_data          = '/gpfs/csfs1/collections/rda/data/'\nrda_url           = 'https://data.rda.ucar.edu/'\n#era5_catalog      = rda_url + 'pythia_intake_catalogs/era5_catalog.json'\n#\nannual_means      =  rda_url + 'pythia_era5_24/annual_means/'\nannual_means_posix=  rda_data + 'pythia_era5_24/annual_means/'\ntemp_annual_means =  annual_means + ''\n#\nprint(annual_means)\n\n","type":"content","url":"/notebooks/era5-anomaly#specify-global-variables","position":9},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl3":"Create a Dask cluster","lvl2":"Imports"},"type":"lvl3","url":"/notebooks/era5-anomaly#create-a-dask-cluster","position":10},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl3":"Create a Dask cluster","lvl2":"Imports"},"content":"\n\n","type":"content","url":"/notebooks/era5-anomaly#create-a-dask-cluster","position":11},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl4":"Dask Introduction","lvl3":"Create a Dask cluster","lvl2":"Imports"},"type":"lvl4","url":"/notebooks/era5-anomaly#dask-introduction","position":12},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl4":"Dask Introduction","lvl3":"Create a Dask cluster","lvl2":"Imports"},"content":"Dask is a solution that enables the scaling of Python libraries. It mimics popular scientific libraries such as numpy, pandas, and xarray that enables an easier path to parallel processing without having to refactor code.\n\nThere are 3 components to parallel processing with Dask: the client, the scheduler, and the workers.\n\nThe Client is best envisioned as the application that sends information to the Dask cluster. In Python applications this is handled when the client is defined with client = Client(CLUSTER_TYPE). A Dask cluster comprises of a single scheduler that manages the execution of tasks on workers. The CLUSTER_TYPE can be defined in a number of different ways.\n\nThere is LocalCluster, a cluster running on the same hardware as the application and sharing the available resources, directly in Python with dask.distributed.\n\nIn certain JupyterHubs Dask Gateway may be available and a dedicated dask cluster with its own resources can be created dynamically with dask.gateway.\n\nOn HPC systems dask_jobqueue is used to connect to the HPC Slurm and PBS job schedulers to provision resources.\n\nThe dask.distributed client python module can also be used to connect to existing clusters. A Dask Scheduler and Workers can be deployed in containers, or on Kubernetes, without using a Python function to create a dask cluster. The dask.distributed Client is configured to connect to the scheduler either by container name, or by the Kubernetes service name.\n\n","type":"content","url":"/notebooks/era5-anomaly#dask-introduction","position":13},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl4":"Select the Dask cluster type","lvl3":"Create a Dask cluster","lvl2":"Imports"},"type":"lvl4","url":"/notebooks/era5-anomaly#select-the-dask-cluster-type","position":14},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl4":"Select the Dask cluster type","lvl3":"Create a Dask cluster","lvl2":"Imports"},"content":"\n\nThe default will be LocalCluster as that can run on any system.\n\nIf running on a HPC computer with a PBS Scheduler, set to True. Otherwise, set to False.\n\nUSE_PBS_SCHEDULER = False\n\nIf running on Jupyter server with Dask Gateway configured, set to True. Otherwise, set to False.\n\nUSE_DASK_GATEWAY = False\n\nPython function for a PBS Cluster\n\n# Create a PBS cluster object\ndef get_pbs_cluster():\n    \"\"\" Create cluster through dask_jobqueue.   \n    \"\"\"\n    from dask_jobqueue import PBSCluster\n    cluster = PBSCluster(\n        job_name = 'dask-pythia-24',\n        cores = 1,\n        memory = '4GiB',\n        processes = 1,\n        local_directory = rda_scratch + '/dask/spill',\n        resource_spec = 'select=1:ncpus=1:mem=4GB',\n        queue = 'casper',\n        walltime = '1:00:00',\n        #interface = 'ib0'\n        interface = 'ext'\n    )\n    return cluster\n\nPython function for a Gateway Cluster\n\ndef get_gateway_cluster():\n    \"\"\" Create cluster through dask_gateway\n    \"\"\"\n    from dask_gateway import Gateway\n\n    gateway = Gateway()\n    cluster = gateway.new_cluster()\n    cluster.adapt(minimum=2, maximum=4)\n    return cluster\n\nPython function for a Local Cluster\n\ndef get_local_cluster():\n    \"\"\" Create cluster using the Jupyter server's resources\n    \"\"\"\n    from distributed import LocalCluster, performance_report\n    cluster = LocalCluster()    \n\n    cluster.scale(4)\n    return cluster\n\nPython logic to select the Dask Cluster type\n\nThis uses True/False boolean logic based on the variables set in the previous cells\n\n# Obtain dask cluster in one of three ways\n\nif USE_PBS_SCHEDULER:\n    cluster = get_pbs_cluster()\nelif USE_DASK_GATEWAY:\n    cluster = get_gateway_cluster()\nelse:\n    cluster = get_local_cluster()\n\n# Connect to cluster\nfrom distributed import Client\nclient = Client(cluster)\n\n# Display cluster dashboard URL\ncluster\n\n","type":"content","url":"/notebooks/era5-anomaly#select-the-dask-cluster-type","position":15},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl3":"Open ERA5 annual means file","lvl2":"Imports"},"type":"lvl3","url":"/notebooks/era5-anomaly#open-era5-annual-means-file","position":16},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl3":"Open ERA5 annual means file","lvl2":"Imports"},"content":"\n\nbaseline_temp = temp_2m_annual.sel(time= \\\n                                   temp_2m_annual.time.dt.year.isin(range(baseline_year_start, baseline_year_end+1))).mean('time')\nbaseline_temp \n\ntemp_anomaly = temp_2m_annual - baseline_temp\ntemp_anomaly\n\n","type":"content","url":"/notebooks/era5-anomaly#open-era5-annual-means-file","position":17},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl3":"Save anomaly","lvl2":"Imports"},"type":"lvl3","url":"/notebooks/era5-anomaly#save-anomaly","position":18},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl3":"Save anomaly","lvl2":"Imports"},"content":"\n\n# %%time\n# temp_anomaly.to_dataset().to_zarr(annual_means_posix + 'temp_anomaly_wrt_1940_1950.zarr')\n\ntemp_anomaly = xr.open_zarr(annual_means_posix + 'temp_anomaly_wrt_1940_1950.zarr').VAR_2T\ntemp_anomaly\n\nPlot the temperature anomaly\n\ntemp_anomaly.isel(time=83).plot()\n\n","type":"content","url":"/notebooks/era5-anomaly#save-anomaly","position":19},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl2":"Close the Dask Cluster"},"type":"lvl2","url":"/notebooks/era5-anomaly#close-the-dask-cluster","position":20},{"hierarchy":{"lvl1":"Access ERA5 data from NCAR’s Research Data Archive and compute anomaly","lvl2":"Close the Dask Cluster"},"content":"It’s best practice to close the Dask cluster when it’s no longer needed to free up the compute resources used.\n\ncluster.close()","type":"content","url":"/notebooks/era5-anomaly#close-the-dask-cluster","position":21},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"type":"lvl1","url":"/notebooks/how-to-cite","position":0},{"hierarchy":{"lvl1":"How to Cite This Cookbook"},"content":"The material in Project Pythia’s ARCO ERA-5 Interactive Visualization Cookbook is licensed for free and open consumption and reuse. All code is served under \n\nApache 2.0, while all non-code content is licensed under \n\nCreative Commons BY 4.0 (CC BY 4.0). Effectively, this means you are free to share and adapt this material so long as you give appropriate credit to the Cookbook authors and the Project Pythia community.\n\nThe source code for the book is \n\nreleased on GitHub and archived on Zenodo. This DOI will always resolve to the latest release of the book source:\n\n","type":"content","url":"/notebooks/how-to-cite","position":1}]}